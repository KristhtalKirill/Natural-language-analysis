{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Лабораторная работа №2\n",
    "\n",
    "**Требования:**\n",
    "* Python >= 3.X\n",
    "\n",
    "Лабораторную работу необходимо выполнять в данном шаблоне. Результатом работы будет являться файл (с измененным именем), который необходимо выложить в Moodle.\n",
    "\n",
    "**Важно!!!** Имя файлу задавайте по следующему шаблону **lab_2_6231-010402D_Кришталь К.Е.ipynb**. Например: если Вас зовут Иванов Иван Иванович, и Вы обучаетесь в группе 6207_010302D, то имя файла будет выглядеть так **lab_2_6231-010402D_Кришталь К.Е..ipynb**.\n",
    "\n",
    "Необходимо провести исследование различных способов представления документов и их влияние на качество определения тональности.\n",
    "\n",
    "В качестве входных данных к лабораторной работе взят широко известный набор данных IMDB, содержащий 50K обзоров фильмов ([imdb-dataset-of-50k-movie-reviews](https://disk.yandex.ru/i/DDb0zuyUmts5QA)). Откликами являются значения двух классов positive и negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading grpcio-1.68.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)\n",
      "   ---------------------------------------- 0.0/390.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/390.3 MB 4.2 MB/s eta 0:01:33\n",
      "   ---------------------------------------- 2.1/390.3 MB 6.2 MB/s eta 0:01:03\n",
      "   ---------------------------------------- 4.7/390.3 MB 8.6 MB/s eta 0:00:45\n",
      "    --------------------------------------- 6.3/390.3 MB 8.4 MB/s eta 0:00:46\n",
      "    --------------------------------------- 8.4/390.3 MB 8.8 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 11.0/390.3 MB 9.3 MB/s eta 0:00:41\n",
      "   - -------------------------------------- 13.4/390.3 MB 9.8 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 16.0/390.3 MB 10.1 MB/s eta 0:00:38\n",
      "   - -------------------------------------- 17.8/390.3 MB 10.2 MB/s eta 0:00:37\n",
      "   -- ------------------------------------- 21.0/390.3 MB 10.4 MB/s eta 0:00:36\n",
      "   -- ------------------------------------- 23.3/390.3 MB 10.5 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 26.0/390.3 MB 10.7 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 28.6/390.3 MB 10.8 MB/s eta 0:00:34\n",
      "   --- ------------------------------------ 31.2/390.3 MB 10.8 MB/s eta 0:00:34\n",
      "   --- ------------------------------------ 33.6/390.3 MB 10.9 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 36.2/390.3 MB 10.9 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 38.8/390.3 MB 11.0 MB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 41.4/390.3 MB 11.1 MB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 43.8/390.3 MB 11.1 MB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 46.4/390.3 MB 11.1 MB/s eta 0:00:31\n",
      "   ----- ---------------------------------- 49.0/390.3 MB 11.2 MB/s eta 0:00:31\n",
      "   ----- ---------------------------------- 51.4/390.3 MB 11.2 MB/s eta 0:00:31\n",
      "   ----- ---------------------------------- 54.0/390.3 MB 11.2 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 56.6/390.3 MB 11.3 MB/s eta 0:00:30\n",
      "   ------ --------------------------------- 59.0/390.3 MB 11.3 MB/s eta 0:00:30\n",
      "   ------ --------------------------------- 61.6/390.3 MB 11.3 MB/s eta 0:00:30\n",
      "   ------ --------------------------------- 64.2/390.3 MB 11.3 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 66.8/390.3 MB 11.4 MB/s eta 0:00:29\n",
      "   ------- -------------------------------- 69.2/390.3 MB 11.4 MB/s eta 0:00:29\n",
      "   ------- -------------------------------- 71.6/390.3 MB 11.4 MB/s eta 0:00:29\n",
      "   ------- -------------------------------- 74.2/390.3 MB 11.4 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 76.8/390.3 MB 11.4 MB/s eta 0:00:28\n",
      "   -------- ------------------------------- 79.2/390.3 MB 11.4 MB/s eta 0:00:28\n",
      "   -------- ------------------------------- 81.8/390.3 MB 11.4 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 84.4/390.3 MB 11.5 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 87.0/390.3 MB 11.5 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 89.4/390.3 MB 11.5 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 91.2/390.3 MB 11.5 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 94.1/390.3 MB 11.5 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 96.5/390.3 MB 11.4 MB/s eta 0:00:26\n",
      "   ---------- ----------------------------- 98.8/390.3 MB 11.4 MB/s eta 0:00:26\n",
      "   ---------- ---------------------------- 101.4/390.3 MB 11.4 MB/s eta 0:00:26\n",
      "   ---------- ---------------------------- 103.8/390.3 MB 11.4 MB/s eta 0:00:26\n",
      "   ---------- ---------------------------- 106.2/390.3 MB 11.4 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 108.8/390.3 MB 11.5 MB/s eta 0:00:25\n",
      "   ----------- --------------------------- 111.4/390.3 MB 11.5 MB/s eta 0:00:25\n",
      "   ----------- --------------------------- 113.8/390.3 MB 11.5 MB/s eta 0:00:25\n",
      "   ----------- --------------------------- 116.1/390.3 MB 11.5 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 118.5/390.3 MB 11.5 MB/s eta 0:00:24\n",
      "   ------------ -------------------------- 120.6/390.3 MB 11.4 MB/s eta 0:00:24\n",
      "   ------------ -------------------------- 122.2/390.3 MB 11.3 MB/s eta 0:00:24\n",
      "   ------------ -------------------------- 123.5/390.3 MB 11.3 MB/s eta 0:00:24\n",
      "   ------------ -------------------------- 125.0/390.3 MB 11.2 MB/s eta 0:00:24\n",
      "   ------------ -------------------------- 126.6/390.3 MB 11.1 MB/s eta 0:00:24\n",
      "   ------------ -------------------------- 128.2/390.3 MB 11.0 MB/s eta 0:00:24\n",
      "   ------------ -------------------------- 129.5/390.3 MB 11.0 MB/s eta 0:00:24\n",
      "   ------------- ------------------------- 131.1/390.3 MB 10.9 MB/s eta 0:00:24\n",
      "   ------------- ------------------------- 132.9/390.3 MB 10.8 MB/s eta 0:00:24\n",
      "   ------------- ------------------------- 134.2/390.3 MB 10.8 MB/s eta 0:00:24\n",
      "   ------------- ------------------------- 136.1/390.3 MB 10.7 MB/s eta 0:00:24\n",
      "   ------------- ------------------------- 137.6/390.3 MB 10.7 MB/s eta 0:00:24\n",
      "   ------------- ------------------------- 139.2/390.3 MB 10.6 MB/s eta 0:00:24\n",
      "   -------------- ------------------------ 141.0/390.3 MB 10.6 MB/s eta 0:00:24\n",
      "   -------------- ------------------------ 142.6/390.3 MB 10.6 MB/s eta 0:00:24\n",
      "   -------------- ------------------------ 144.4/390.3 MB 10.5 MB/s eta 0:00:24\n",
      "   -------------- ------------------------ 146.0/390.3 MB 10.5 MB/s eta 0:00:24\n",
      "   -------------- ------------------------ 147.8/390.3 MB 10.4 MB/s eta 0:00:24\n",
      "   -------------- ------------------------ 149.4/390.3 MB 10.4 MB/s eta 0:00:24\n",
      "   --------------- ----------------------- 151.0/390.3 MB 10.4 MB/s eta 0:00:24\n",
      "   --------------- ----------------------- 152.8/390.3 MB 10.3 MB/s eta 0:00:24\n",
      "   --------------- ----------------------- 154.4/390.3 MB 10.3 MB/s eta 0:00:23\n",
      "   --------------- ----------------------- 156.2/390.3 MB 10.2 MB/s eta 0:00:23\n",
      "   --------------- ----------------------- 157.8/390.3 MB 10.2 MB/s eta 0:00:23\n",
      "   --------------- ----------------------- 159.6/390.3 MB 10.2 MB/s eta 0:00:23\n",
      "   ---------------- ---------------------- 161.2/390.3 MB 10.2 MB/s eta 0:00:23\n",
      "   ---------------- ---------------------- 163.1/390.3 MB 10.1 MB/s eta 0:00:23\n",
      "   ---------------- ---------------------- 164.9/390.3 MB 10.1 MB/s eta 0:00:23\n",
      "   ---------------- ---------------------- 166.5/390.3 MB 10.1 MB/s eta 0:00:23\n",
      "   ---------------- ---------------------- 168.0/390.3 MB 10.1 MB/s eta 0:00:23\n",
      "   ---------------- ---------------------- 169.9/390.3 MB 10.0 MB/s eta 0:00:22\n",
      "   ----------------- --------------------- 171.7/390.3 MB 10.0 MB/s eta 0:00:22\n",
      "   ----------------- --------------------- 173.3/390.3 MB 10.0 MB/s eta 0:00:22\n",
      "   ----------------- --------------------- 175.1/390.3 MB 10.0 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 176.7/390.3 MB 9.9 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 178.5/390.3 MB 9.9 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 180.4/390.3 MB 9.9 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 182.2/390.3 MB 9.9 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 184.0/390.3 MB 9.9 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 185.9/390.3 MB 9.9 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 187.7/390.3 MB 9.9 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 189.5/390.3 MB 9.8 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 191.4/390.3 MB 9.8 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 193.2/390.3 MB 9.8 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 195.0/390.3 MB 9.8 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 197.1/390.3 MB 9.8 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 199.0/390.3 MB 9.8 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 200.8/390.3 MB 9.8 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 202.6/390.3 MB 9.8 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 204.7/390.3 MB 9.8 MB/s eta 0:00:19\n",
      "   --------------------- ------------------ 206.8/390.3 MB 9.8 MB/s eta 0:00:19\n",
      "   --------------------- ------------------ 208.9/390.3 MB 9.8 MB/s eta 0:00:19\n",
      "   --------------------- ------------------ 211.0/390.3 MB 9.8 MB/s eta 0:00:19\n",
      "   --------------------- ------------------ 213.1/390.3 MB 9.8 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 215.5/390.3 MB 9.8 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 217.8/390.3 MB 9.8 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 220.2/390.3 MB 9.8 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 222.8/390.3 MB 9.8 MB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 225.2/390.3 MB 9.9 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 227.5/390.3 MB 9.9 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 230.2/390.3 MB 9.9 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 232.8/390.3 MB 9.9 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 235.4/390.3 MB 9.9 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 237.8/390.3 MB 9.9 MB/s eta 0:00:16\n",
      "   ------------------------ -------------- 240.4/390.3 MB 10.0 MB/s eta 0:00:16\n",
      "   ------------------------ -------------- 243.0/390.3 MB 10.0 MB/s eta 0:00:15\n",
      "   ------------------------ -------------- 245.6/390.3 MB 10.0 MB/s eta 0:00:15\n",
      "   ------------------------ -------------- 248.0/390.3 MB 10.0 MB/s eta 0:00:15\n",
      "   ------------------------- ------------- 250.6/390.3 MB 10.0 MB/s eta 0:00:14\n",
      "   ------------------------- ------------- 253.2/390.3 MB 10.0 MB/s eta 0:00:14\n",
      "   ------------------------- ------------- 255.6/390.3 MB 10.1 MB/s eta 0:00:14\n",
      "   ------------------------- ------------- 258.2/390.3 MB 10.1 MB/s eta 0:00:14\n",
      "   -------------------------- ------------ 260.8/390.3 MB 10.1 MB/s eta 0:00:13\n",
      "   -------------------------- ------------ 263.2/390.3 MB 10.1 MB/s eta 0:00:13\n",
      "   -------------------------- ------------ 265.8/390.3 MB 10.2 MB/s eta 0:00:13\n",
      "   -------------------------- ------------ 268.2/390.3 MB 10.2 MB/s eta 0:00:12\n",
      "   --------------------------- ----------- 270.8/390.3 MB 10.2 MB/s eta 0:00:12\n",
      "   --------------------------- ----------- 273.4/390.3 MB 10.2 MB/s eta 0:00:12\n",
      "   --------------------------- ----------- 275.8/390.3 MB 10.2 MB/s eta 0:00:12\n",
      "   --------------------------- ----------- 278.4/390.3 MB 10.2 MB/s eta 0:00:11\n",
      "   ---------------------------- ---------- 281.0/390.3 MB 10.2 MB/s eta 0:00:11\n",
      "   ---------------------------- ---------- 283.4/390.3 MB 10.2 MB/s eta 0:00:11\n",
      "   ---------------------------- ---------- 286.0/390.3 MB 10.2 MB/s eta 0:00:11\n",
      "   ---------------------------- ---------- 288.4/390.3 MB 10.2 MB/s eta 0:00:11\n",
      "   ----------------------------- --------- 291.0/390.3 MB 10.2 MB/s eta 0:00:10\n",
      "   ----------------------------- --------- 293.6/390.3 MB 10.2 MB/s eta 0:00:10\n",
      "   ----------------------------- --------- 296.2/390.3 MB 10.2 MB/s eta 0:00:10\n",
      "   ----------------------------- --------- 298.6/390.3 MB 10.2 MB/s eta 0:00:09\n",
      "   ------------------------------ -------- 300.9/390.3 MB 10.2 MB/s eta 0:00:09\n",
      "   ------------------------------ -------- 303.6/390.3 MB 10.2 MB/s eta 0:00:09\n",
      "   ------------------------------ -------- 306.2/390.3 MB 10.2 MB/s eta 0:00:09\n",
      "   ------------------------------ -------- 308.5/390.3 MB 10.2 MB/s eta 0:00:09\n",
      "   ------------------------------- ------- 311.2/390.3 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------------------------- ------- 313.8/390.3 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------------------------- ------- 316.1/390.3 MB 10.2 MB/s eta 0:00:08\n",
      "   ------------------------------- ------- 318.8/390.3 MB 10.2 MB/s eta 0:00:08\n",
      "   -------------------------------- ------ 321.4/390.3 MB 10.2 MB/s eta 0:00:07\n",
      "   -------------------------------- ------ 323.5/390.3 MB 10.2 MB/s eta 0:00:07\n",
      "   -------------------------------- ------ 325.8/390.3 MB 10.2 MB/s eta 0:00:07\n",
      "   -------------------------------- ------ 328.5/390.3 MB 10.2 MB/s eta 0:00:07\n",
      "   --------------------------------- ----- 331.1/390.3 MB 10.2 MB/s eta 0:00:06\n",
      "   --------------------------------- ----- 333.4/390.3 MB 10.2 MB/s eta 0:00:06\n",
      "   --------------------------------- ----- 336.1/390.3 MB 10.2 MB/s eta 0:00:06\n",
      "   --------------------------------- ----- 338.7/390.3 MB 10.2 MB/s eta 0:00:06\n",
      "   ---------------------------------- ---- 341.3/390.3 MB 10.2 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 343.7/390.3 MB 10.2 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 346.3/390.3 MB 10.2 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 348.4/390.3 MB 10.2 MB/s eta 0:00:05\n",
      "   ----------------------------------- --- 350.5/390.3 MB 10.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 352.8/390.3 MB 10.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 354.9/390.3 MB 10.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 357.3/390.3 MB 10.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 359.7/390.3 MB 10.1 MB/s eta 0:00:04\n",
      "   ------------------------------------ -- 362.0/390.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 364.4/390.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 366.7/390.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 369.4/390.3 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- - 371.7/390.3 MB 10.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 374.3/390.3 MB 10.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 376.7/390.3 MB 10.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 379.1/390.3 MB 10.1 MB/s eta 0:00:02\n",
      "   --------------------------------------  381.7/390.3 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  384.0/390.3 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  386.7/390.3 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.0/390.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 390.3/390.3 MB 9.8 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.68.1-cp312-cp312-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 2.4/4.4 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.4/4.4 MB 11.0 MB/s eta 0:00:00\n",
      "Downloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.4/26.4 MB 12.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 5.0/26.4 MB 12.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 7.1/26.4 MB 11.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 9.7/26.4 MB 11.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 12.3/26.4 MB 11.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.7/26.4 MB 11.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.3/26.4 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.7/26.4 MB 11.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 22.3/26.4 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.6/26.4 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.4 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 10.9 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 2.1/5.5 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.7/5.5 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 10.8 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp312-cp312-win_amd64.whl (292 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.1 keras-3.7.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 termcolor-2.5.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Код загрузки данных\n",
    "# Если хотите добавить какие-либо библиотеки\n",
    "# добавляйте их ИМЕННО ЗДЕСЬ\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "imdb_data = pd.read_csv(r'input/IMDB Dataset.csv')\n",
    "imdb_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг №1 Подготовка данных\n",
    "\n",
    "Обязательно предобработайте данные!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>review_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>one reviewers mentioned watching oz episode yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a wonderful little production br br the filmin...</td>\n",
       "      <td>wonderful little production br br filming tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically theres a family where a little boy j...</td>\n",
       "      <td>basically theres family little boy jake thinks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>This movie really woke me up, like it wakes up...</td>\n",
       "      <td>positive</td>\n",
       "      <td>this movie really woke me up like it wakes up ...</td>\n",
       "      <td>movie really woke like wakes main male charact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Ed Wood rides again. The fact that this movie ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ed wood rides again the fact that this movie w...</td>\n",
       "      <td>ed wood rides fact movie made give youngbr br ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>This sports a nice, deep cast but for a thrill...</td>\n",
       "      <td>negative</td>\n",
       "      <td>this sports a nice deep cast but for a thrille...</td>\n",
       "      <td>sports nice deep cast thriller better deliver ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>I think this movie has got it all. It has real...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i think this movie has got it all it has reall...</td>\n",
       "      <td>think movie got really cool music never get he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Howard (Kevin Kline) teaches English at the hi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>howard kevin kline teaches english at the high...</td>\n",
       "      <td>howard kevin kline teaches english high school...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment  \\\n",
       "0    One of the other reviewers has mentioned that ...  positive   \n",
       "1    A wonderful little production. <br /><br />The...  positive   \n",
       "2    I thought this was a wonderful way to spend ti...  positive   \n",
       "3    Basically there's a family where a little boy ...  negative   \n",
       "4    Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "..                                                 ...       ...   \n",
       "218  This movie really woke me up, like it wakes up...  positive   \n",
       "219  Ed Wood rides again. The fact that this movie ...  negative   \n",
       "220  This sports a nice, deep cast but for a thrill...  negative   \n",
       "221  I think this movie has got it all. It has real...  positive   \n",
       "222  Howard (Kevin Kline) teaches English at the hi...  positive   \n",
       "\n",
       "                                        cleaned_review  \\\n",
       "0    one of the other reviewers has mentioned that ...   \n",
       "1    a wonderful little production br br the filmin...   \n",
       "2    i thought this was a wonderful way to spend ti...   \n",
       "3    basically theres a family where a little boy j...   \n",
       "4    petter matteis love in the time of money is a ...   \n",
       "..                                                 ...   \n",
       "218  this movie really woke me up like it wakes up ...   \n",
       "219  ed wood rides again the fact that this movie w...   \n",
       "220  this sports a nice deep cast but for a thrille...   \n",
       "221  i think this movie has got it all it has reall...   \n",
       "222  howard kevin kline teaches english at the high...   \n",
       "\n",
       "                              review_without_stopwords  \n",
       "0    one reviewers mentioned watching oz episode yo...  \n",
       "1    wonderful little production br br filming tech...  \n",
       "2    thought wonderful way spend time hot summer we...  \n",
       "3    basically theres family little boy jake thinks...  \n",
       "4    petter matteis love time money visually stunni...  \n",
       "..                                                 ...  \n",
       "218  movie really woke like wakes main male charact...  \n",
       "219  ed wood rides fact movie made give youngbr br ...  \n",
       "220  sports nice deep cast thriller better deliver ...  \n",
       "221  think movie got really cool music never get he...  \n",
       "222  howard kevin kline teaches english high school...  \n",
       "\n",
       "[223 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вставьте код здесь\n",
    "\n",
    "\n",
    "# Проверка на пустые строки и удаление\n",
    "imdb_data = imdb_data.dropna(subset=['review'])\n",
    "\n",
    "# Функция для очистки текста\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Удаляем цифры, знаки препинания,приводим к нижнему регистру\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # оставляем только буквы\n",
    "    text = text.lower()  # делаем маленькие буквы\n",
    "    return text\n",
    "\n",
    "# очищаем все отзывы\n",
    "imdb_data['cleaned_review'] = imdb_data['review'].apply(clean_text)\n",
    "\n",
    "# Удаление стоп-слов\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # разбиваем текст на слова\n",
    "    words_filtered = [word for word in words if word not in stop_words]  # удаляем стоп-слова\n",
    "    return ' '.join(words_filtered)  #  обратно в строку\n",
    "\n",
    "imdb_data['review_without_stopwords'] = imdb_data['cleaned_review'].apply(remove_stopwords)\n",
    "\n",
    "# Лемматизация\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#def lemmatize_text(text):\n",
    " #   words = text.split()  # разбиваем текст на слова\n",
    "  #  lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # лемматизируем каждое слово\n",
    "   # return ' '.join(lemmatized_words)  # соединяем обратно в строку\n",
    "\n",
    "# imdb_data['lemmatized_review'] = imdb_data['review_without_stopwords'].apply(lemmatize_text)\n",
    "\n",
    "\n",
    "imdb_data.head(223)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При векторизации ограничьте количество признаков до 500. В качестве исследуемых способов векторизации текстов необходимо рассмотреть:\n",
    "\n",
    "#### 1. Компоненты вектора: частоты ([CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actress</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>writer</th>\n",
       "      <th>writing</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     able  absolutely  across  act  acting  action  actor  actress  actually  \\\n",
       "0       0           0       0    0       0       0      0        0         0   \n",
       "1       0           0       0    0       0       0      1        0         0   \n",
       "2       0           0       0    0       0       0      0        0         0   \n",
       "3       0           0       0    0       0       0      0        0         0   \n",
       "4       0           0       0    0       1       1      0        0         0   \n",
       "..    ...         ...     ...  ...     ...     ...    ...      ...       ...   \n",
       "218     0           0       0    0       2       1      0        0         0   \n",
       "219     0           0       0    0       0       0      1        0         0   \n",
       "220     0           0       0    0       0       0      2        0         0   \n",
       "221     0           0       0    0       1       0      1        0         0   \n",
       "222     1           0       0    0       1       0      0        0         0   \n",
       "\n",
       "     add  ...  writer  writing  written  wrong  year  yes  yet  youll  young  \\\n",
       "0      0  ...       0        0        0      0     0    0    0      1      0   \n",
       "1      0  ...       0        0        1      0     0    0    0      0      0   \n",
       "2      0  ...       0        0        0      0     1    0    0      0      1   \n",
       "3      0  ...       0        0        0      0     0    0    0      0      0   \n",
       "4      0  ...       0        0        0      0     0    0    0      0      0   \n",
       "..   ...  ...     ...      ...      ...    ...   ...  ...  ...    ...    ...   \n",
       "218    0  ...       1        0        0      0     0    0    0      0      0   \n",
       "219    0  ...       0        0        0      0     0    0    0      0      0   \n",
       "220    0  ...       0        0        0      0     1    0    0      0      0   \n",
       "221    0  ...       0        0        0      0     0    0    0      2      0   \n",
       "222    0  ...       0        0        0      0     0    1    0      0      0   \n",
       "\n",
       "     youre  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        1  \n",
       "4        0  \n",
       "..     ...  \n",
       "218      1  \n",
       "219      0  \n",
       "220      0  \n",
       "221      0  \n",
       "222      0  \n",
       "\n",
       "[223 rows x 500 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вставьте код здесь\n",
    "\n",
    "#  500 признаков\n",
    "vectorizer = CountVectorizer(max_features=500)\n",
    "\n",
    "# 'lemmatized_review' Юзаем как чистые данные \n",
    "X = vectorizer.fit_transform(imdb_data['lemmatized_review'])\n",
    "\n",
    "# Делаем DataFrame \n",
    "X_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "X_df.head(223)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Компоненты вектора: оценки tf-idf для слова ([TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actress</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>writer</th>\n",
       "      <th>writing</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150741</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073014</td>\n",
       "      <td>0.096033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111198</td>\n",
       "      <td>0.073128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.131483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         able  absolutely  across  act    acting    action     actor  actress  \\\n",
       "0    0.000000         0.0     0.0  0.0  0.000000  0.000000  0.000000      0.0   \n",
       "1    0.000000         0.0     0.0  0.0  0.000000  0.000000  0.101022      0.0   \n",
       "2    0.000000         0.0     0.0  0.0  0.000000  0.000000  0.000000      0.0   \n",
       "3    0.000000         0.0     0.0  0.0  0.000000  0.000000  0.000000      0.0   \n",
       "4    0.000000         0.0     0.0  0.0  0.073014  0.096033  0.000000      0.0   \n",
       "..        ...         ...     ...  ...       ...       ...       ...      ...   \n",
       "218  0.000000         0.0     0.0  0.0  0.111198  0.073128  0.000000      0.0   \n",
       "219  0.000000         0.0     0.0  0.0  0.000000  0.000000  0.124656      0.0   \n",
       "220  0.000000         0.0     0.0  0.0  0.000000  0.000000  0.204114      0.0   \n",
       "221  0.000000         0.0     0.0  0.0  0.064143  0.000000  0.065651      0.0   \n",
       "222  0.131483         0.0     0.0  0.0  0.082887  0.000000  0.000000      0.0   \n",
       "\n",
       "     actually  add  ...    writer  writing   written  wrong      year  \\\n",
       "0         0.0  0.0  ...  0.000000      0.0  0.000000    0.0  0.000000   \n",
       "1         0.0  0.0  ...  0.000000      0.0  0.152431    0.0  0.000000   \n",
       "2         0.0  0.0  ...  0.000000      0.0  0.000000    0.0  0.124243   \n",
       "3         0.0  0.0  ...  0.000000      0.0  0.000000    0.0  0.000000   \n",
       "4         0.0  0.0  ...  0.000000      0.0  0.000000    0.0  0.000000   \n",
       "..        ...  ...  ...       ...      ...       ...    ...       ...   \n",
       "218       0.0  0.0  ...  0.087355      0.0  0.000000    0.0  0.000000   \n",
       "219       0.0  0.0  ...  0.000000      0.0  0.000000    0.0  0.000000   \n",
       "220       0.0  0.0  ...  0.000000      0.0  0.000000    0.0  0.104305   \n",
       "221       0.0  0.0  ...  0.000000      0.0  0.000000    0.0  0.000000   \n",
       "222       0.0  0.0  ...  0.000000      0.0  0.000000    0.0  0.000000   \n",
       "\n",
       "          yes  yet     youll     young     youre  \n",
       "0    0.000000  0.0  0.111406  0.000000  0.000000  \n",
       "1    0.000000  0.0  0.000000  0.000000  0.000000  \n",
       "2    0.000000  0.0  0.000000  0.150741  0.000000  \n",
       "3    0.000000  0.0  0.000000  0.000000  0.172866  \n",
       "4    0.000000  0.0  0.000000  0.000000  0.000000  \n",
       "..        ...  ...       ...       ...       ...  \n",
       "218  0.000000  0.0  0.000000  0.000000  0.081606  \n",
       "219  0.000000  0.0  0.000000  0.000000  0.000000  \n",
       "220  0.000000  0.0  0.000000  0.000000  0.000000  \n",
       "221  0.000000  0.0  0.204256  0.000000  0.000000  \n",
       "222  0.129319  0.0  0.000000  0.000000  0.000000  \n",
       "\n",
       "[223 rows x 500 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(imdb_data['lemmatized_review'])\n",
    "\n",
    "#  Делаем DataFrame \n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "X_tfidf_df.head(223)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Компоненты вектора: частоты N-грам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lemmatized_review'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lemmatized_review'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m vectorizer_ngram \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#  'lemmatized_review' как уже очищенные данные\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X_ngram \u001b[38;5;241m=\u001b[39m vectorizer_ngram\u001b[38;5;241m.\u001b[39mfit_transform(imdb_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized_review\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#  DataFrame \u001b[39;00m\n\u001b[0;32m     10\u001b[0m X_ngram_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_ngram\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer_ngram\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lemmatized_review'"
     ]
    }
   ],
   "source": [
    "# вставьте код здесь\n",
    "\n",
    "# Векторизация текста с униграммы и биграммы\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(2, 2), max_features=500)\n",
    "\n",
    "#  'lemmatized_review' как уже очищенные данные\n",
    "X_ngram = vectorizer_ngram.fit_transform(imdb_data['lemmatized_review'])\n",
    "\n",
    "#  DataFrame \n",
    "X_ngram_df = pd.DataFrame(X_ngram.toarray(), columns=vectorizer_ngram.get_feature_names_out())\n",
    "\n",
    "# Посмотрим на первые 5 строк\n",
    "X_ngram_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2. Исследование моделей\n",
    "\n",
    "Матрица ошибок (confusion matrix):\n",
    "\n",
    "<table>\n",
    "\t\t<tr>\n",
    "\t\t\t<td></td>\n",
    "\t\t\t<td>$y = 1$</td>\n",
    "\t\t\t<td>$y = 0$</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>$a(x) = 1$</td>\n",
    "\t\t\t<td>True Positive (TP)</td>\n",
    "\t\t\t<td>False Positive (FP)</td>\n",
    "\t\t</tr>\n",
    "    \t<tr>\n",
    "\t\t\t<td>$a(x) = 0$</td>\n",
    "\t\t\t<td>False Negative (FN)</td>\n",
    "\t\t\t<td>True Negative (TN)</td>\n",
    "\t\t</tr>\n",
    "</table>\n",
    "\n",
    "Метрики качества классификации:\n",
    "\n",
    "$$\\operatorname{accuracy} = \\frac{\\operatorname{TP} + \\operatorname{TN}}{\\operatorname{TP} + \\operatorname{TN} + \\operatorname{FP} + \\operatorname{FN}}$$\n",
    "\n",
    "\n",
    "$$\\operatorname{precision} = \\frac{\\operatorname{TP}}{\\operatorname{TP} + \\operatorname{FP}}$$\n",
    "\n",
    "$$\\operatorname{recall} = \\frac{\\operatorname{TP}}{\\operatorname{TP} + \\operatorname{FN}}$$\n",
    "\n",
    "$$\\operatorname{F} = \\frac{\\operatorname{precision} \\cdot \\operatorname{recall}}{\\operatorname{precision} + \\operatorname{recall}}$$\n",
    "\n",
    "Для каждой модели и каждого способа векторизации текстов необходимо:\n",
    "\n",
    "1. Определить оптимальные гиперпараметры (по F-мере) ([GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).\n",
    "\n",
    "2. Количество блоков при перекрестной проверке (cross-validation) должно быть равно 3.\n",
    "\n",
    "3. Для ускорения процесса можно ограничиться 30% всех данных.\n",
    "\n",
    "Оценку производим для следующих моделей:\n",
    "\n",
    "#### 1. Машина опорных векторов ([SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "param_grid_svc = {\n",
    "    'C': [0.2, 2, 20],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Случайный лес ([RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты для TF-IDF векторизации:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_splits=3 cannot be greater than the number of members in each class.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m rf_model_tfidf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     37\u001b[0m grid_search_rf_tfidf \u001b[38;5;241m=\u001b[39m GridSearchCV(rf_model_tfidf, param_grid_rf, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m grid_search_rf_tfidf\u001b[38;5;241m.\u001b[39mfit(X_train_tfidf, y_train)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Лучшая модель для TF-IDF\u001b[39;00m\n\u001b[0;32m     41\u001b[0m rf_best_tfidf \u001b[38;5;241m=\u001b[39m grid_search_rf_tfidf\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    966\u001b[0m         clone(base_estimator),\n\u001b[0;32m    967\u001b[0m         X,\n\u001b[0;32m    968\u001b[0m         y,\n\u001b[0;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[1;32m--> 976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    979\u001b[0m     )\n\u001b[0;32m    980\u001b[0m )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:416\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         (\n\u001b[0;32m    411\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[0;32m    414\u001b[0m     )\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:147\u001b[0m, in \u001b[0;36mBaseCrossValidator.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    145\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m    146\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(_num_samples(X))\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_test_masks(X, y, groups):\n\u001b[0;32m    148\u001b[0m     train_index \u001b[38;5;241m=\u001b[39m indices[np\u001b[38;5;241m.\u001b[39mlogical_not(test_index)]\n\u001b[0;32m    149\u001b[0m     test_index \u001b[38;5;241m=\u001b[39m indices[test_index]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:809\u001b[0m, in \u001b[0;36mStratifiedKFold._iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_test_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 809\u001b[0m     test_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_test_folds(X, y)\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits):\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m test_folds \u001b[38;5;241m==\u001b[39m i\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:771\u001b[0m, in \u001b[0;36mStratifiedKFold._make_test_folds\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    769\u001b[0m min_groups \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(y_counts)\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m y_counts):\n\u001b[1;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    772\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_splits=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m cannot be greater than the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of members in each class.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits)\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m min_groups:\n\u001b[0;32m    776\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    777\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    778\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m members, which is less than n_splits=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    779\u001b[0m         \u001b[38;5;241m%\u001b[39m (min_groups, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits),\n\u001b[0;32m    780\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    781\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: n_splits=3 cannot be greater than the number of members in each class."
     ]
    }
   ],
   "source": [
    "#вЗЯТЬ 30% ОТ ПРЕДЕДЫЩИХ ДАННЫХ\n",
    "# Загружаем данные (замените на свой источник данных)\n",
    "# imdb_data = pd.read_csv('path_to_your_file.csv')  # Если данные в CSV\n",
    "# X, y = imdb_data['review'], imdb_data['sentiment']\n",
    "\n",
    "# Для примера: создадим искусственные данные\n",
    "X = [\"This is a great movie\", \"I hated this movie\", \"Amazing acting and direction\", \n",
    "     \"Worst film ever\", \"Loved the plot and characters\", \"Terrible experience watching this\"]\n",
    "y = [\"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\"]\n",
    "\n",
    "# Разделение данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Параметры для GridSearchCV\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# ----------- TF-IDF Векторизация -----------\n",
    "print(\"\\nРезультаты для TF-IDF векторизации:\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Случайный лес с TF-IDF\n",
    "rf_model_tfidf = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf_tfidf = GridSearchCV(rf_model_tfidf, param_grid_rf, scoring='f1_macro', cv=3, n_jobs=-1)\n",
    "grid_search_rf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Лучшая модель для TF-IDF\n",
    "rf_best_tfidf = grid_search_rf_tfidf.best_estimator_\n",
    "y_pred_rf_tfidf = rf_best_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Метрики для TF-IDF\n",
    "print(classification_report(y_test, y_pred_rf_tfidf))\n",
    "print(confusion_matrix(y_test, y_pred_rf_tfidf))\n",
    "print(\"Лучшие параметры для TF-IDF:\", grid_search_rf_tfidf.best_params_)\n",
    "print(\"Лучший результат (F1):\", grid_search_rf_tfidf.best_score_)\n",
    "\n",
    "# ----------- CountVectorizer Векторизация -----------\n",
    "print(\"\\nРезультаты для CountVectorizer векторизации:\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Случайный лес с CountVectorizer\n",
    "rf_model_count = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf_count = GridSearchCV(rf_model_count, param_grid_rf, scoring='f1_macro', cv=3, n_jobs=-1)\n",
    "grid_search_rf_count.fit(X_train_count, y_train)\n",
    "\n",
    "# Лучшая модель для CountVectorizer\n",
    "rf_best_count = grid_search_rf_count.best_estimator_\n",
    "y_pred_rf_count = rf_best_count.predict(X_test_count)\n",
    "\n",
    "# Метрики для CountVectorizer\n",
    "print(classification_report(y_test, y_pred_rf_count))\n",
    "print(confusion_matrix(y_test, y_pred_rf_count))\n",
    "print(\"Лучшие параметры для CountVectorizer:\", grid_search_rf_count.best_params_)\n",
    "print(\"Лучший результат (F1):\", grid_search_rf_count.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],          # Количество деревьев в лесу\n",
    "    'max_depth': [None, 10, 20, 30],         # Глубина деревьев\n",
    "    'min_samples_split': [2, 5, 10],         # Минимальное количество образцов для разделения узла\n",
    "    'min_samples_leaf': [1, 2, 4],           # Минимальное количество образцов в листе\n",
    "    'bootstrap': [True, False]               # Использование бутстрэппинга при построении деревьев\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, scoring='f1', cv=3, n_jobs=-1)\n",
    "\n",
    "grid_search_rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "rf_best = grid_search_rf.best_estimator_\n",
    "\n",
    "y_pred_rf = rf_best.predict(X_test_tfidf)\n",
    "\n",
    "\n",
    "print(\"Метрики качества для случайного леса (TF-IDF):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод лучших гиперпараметров модели\n",
    "print(\"Лучшие параметры для случайного леса:\", grid_search_rf.best_params_)\n",
    "print(\"Лучший результат (F1):\", grid_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Создаем векторизатор для подсчета частоты слов\n",
    "count_vectorizer = CountVectorizer(max_features=500)  # Оставляем только 500 наиболее частых слов\n",
    "\n",
    "# Преобразуем текстовые данные в векторное представление\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Создаем классификатор случайного леса\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, scoring='f1', cv=3, n_jobs=-1)\n",
    "\n",
    "# Обучаем GridSearchCV на обучающих данных (используем X_train_count, который был преобразован с CountVectorizer)\n",
    "grid_search_rf.fit(X_train_count, y_train)\n",
    "\n",
    "# Получаем лучшую модель после поиска по сетке\n",
    "rf_best_model = grid_search_rf.best_estimator_\n",
    "\n",
    "# Прогнозируем на тестовых данных\n",
    "y_pred_rf = rf_best_model.predict(X_test_count)\n",
    "\n",
    "# Оценка модели случайного леса для CountVectorizer\n",
    "print(\"Метрики качества для лучшей модели случайного леса (CountVectorizer):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# Вывод лучших гиперпараметров модели\n",
    "print(\"Лучшие параметры для случайного леса:\", grid_search_rf.best_params_)\n",
    "print(\"Лучший результат (F1):\", grid_search_rf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 3. Сравнение результатов\n",
    "\n",
    "1. Сравнить и найти наиболее точные (по F-мере) модель и способ векторизации текстов.\n",
    "\n",
    "2. Обучить полученную модель (с заданными оптимальными гиперпараметрами) на всех данных (80% обучающая выборка, 20% тестовая выборка).\n",
    "\n",
    "3. На тестовой выборке постоить матрицу ошибок (confusion matrix) и оценить качество классификации как accuracy, precision, recall и F-мера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки (80%/20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(imdb_data['review'], imdb_data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Параметры для поиска по сетке для RandomForestClassifier\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],          # Количество деревьев в лесу\n",
    "    'max_depth': [None, 10, 20, 30],         # Глубина деревьев\n",
    "    'min_samples_split': [2, 5, 10],         # Минимальное количество образцов для разделения узла\n",
    "    'min_samples_leaf': [1, 2, 4],           # Минимальное количество образцов в листе\n",
    "    'bootstrap': [True, False]               # Использование бутстрэппинга при построении деревьев\n",
    "}\n",
    "\n",
    "# 1. Обучение модели для CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf_count = GridSearchCV(rf_model, param_grid_rf, scoring='f1', cv=3, n_jobs=-1)\n",
    "grid_search_rf_count.fit(X_train_count, y_train)\n",
    "\n",
    "# Оценка модели с CountVectorizer\n",
    "best_rf_count = grid_search_rf_count.best_estimator_\n",
    "y_pred_rf_count = best_rf_count.predict(X_test_count)\n",
    "f1_count = grid_search_rf_count.best_score_\n",
    "\n",
    "# 2. Обучение модели для TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "grid_search_rf_tfidf = GridSearchCV(rf_model, param_grid_rf, scoring='f1', cv=3, n_jobs=-1)\n",
    "grid_search_rf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Оценка модели с TfidfVectorizer\n",
    "best_rf_tfidf = grid_search_rf_tfidf.best_estimator_\n",
    "y_pred_rf_tfidf = best_rf_tfidf.predict(X_test_tfidf)\n",
    "f1_tfidf = grid_search_rf_tfidf.best_score_\n",
    "\n",
    "# 3. Обучение модели для N-грамм (например, биграмм)\n",
    "ngram_vectorizer = CountVectorizer(max_features=500, ngram_range=(1, 2))\n",
    "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
    "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
    "\n",
    "grid_search_rf_ngram = GridSearchCV(rf_model, param_grid_rf, scoring='f1', cv=3, n_jobs=-1)\n",
    "grid_search_rf_ngram.fit(X_train_ngram, y_train)\n",
    "\n",
    "# Оценка модели с N-граммами\n",
    "best_rf_ngram = grid_search_rf_ngram.best_estimator_\n",
    "y_pred_rf_ngram = best_rf_ngram.predict(X_test_ngram)\n",
    "f1_ngram = grid_search_rf_ngram.best_score_\n",
    "\n",
    "# Сравнение F1-мер для разных моделей\n",
    "print(f\"F1-мер для CountVectorizer: {f1_count}\")\n",
    "print(f\"F1-мер для TfidfVectorizer: {f1_tfidf}\")\n",
    "print(f\"F1-мер для N-грамм: {f1_ngram}\")\n",
    "\n",
    "# Выбор лучшей модели по F1-мере\n",
    "best_model = None\n",
    "if f1_count > f1_tfidf and f1_count > f1_ngram:\n",
    "    best_model = best_rf_count\n",
    "    best_pred = y_pred_rf_count\n",
    "    best_vectorizer = 'CountVectorizer'\n",
    "elif f1_tfidf > f1_count and f1_tfidf > f1_ngram:\n",
    "    best_model = best_rf_tfidf\n",
    "    best_pred = y_pred_rf_tfidf\n",
    "    best_vectorizer = 'TfidfVectorizer'\n",
    "else:\n",
    "    best_model = best_rf_ngram\n",
    "    best_pred = y_pred_rf_ngram\n",
    "    best_vectorizer = 'N-grams'\n",
    "\n",
    "print(f\"Лучшая модель использует: {best_vectorizer}\")\n",
    "\n",
    "# Обучение лучшей модели на всех данных (80% обучающая выборка, 20% тестовая выборка)\n",
    "X_all = imdb_data['review']\n",
    "y_all = imdb_data['sentiment']\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "# Преобразование с использованием выбранного векторизатора\n",
    "if best_vectorizer == 'CountVectorizer':\n",
    "    X_train_all_vect = count_vectorizer.fit_transform(X_train_all)\n",
    "    X_test_all_vect = count_vectorizer.transform(X_test_all)\n",
    "elif best_vectorizer == 'TfidfVectorizer':\n",
    "    X_train_all_vect = tfidf_vectorizer.fit_transform(X_train_all)\n",
    "    X_test_all_vect = tfidf_vectorizer.transform(X_test_all)\n",
    "else:  # N-grams\n",
    "    X_train_all_vect = ngram_vectorizer.fit_transform(X_train_all)\n",
    "    X_test_all_vect = ngram_vectorizer.transform(X_test_all)\n",
    "\n",
    "best_model.fit(X_train_all_vect, y_train_all)\n",
    "y_pred_all = best_model.predict(X_test_all_vect)\n",
    "\n",
    "# Оценка качества на тестовой выборке\n",
    "print(\"Метрики качества для лучшей модели:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_all, y_pred_all)}\")\n",
    "print(classification_report(y_test_all, y_pred_all))\n",
    "print(confusion_matrix(y_test_all, y_pred_all))\n",
    "\n",
    "# Построение матрицы ошибок (confusion matrix)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(confusion_matrix(y_test_all, y_pred_all), annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
