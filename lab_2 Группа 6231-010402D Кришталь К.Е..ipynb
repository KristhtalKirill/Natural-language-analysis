{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Лабораторная работа №2\n",
    "\n",
    "**Требования:**\n",
    "* Python >= 3.X\n",
    "\n",
    "Лабораторную работу необходимо выполнять в данном шаблоне. Результатом работы будет являться файл (с измененным именем), который необходимо выложить в Moodle.\n",
    "\n",
    "**Важно!!!** Имя файлу задавайте по следующему шаблону **lab_2_6231-010402D_Кришталь К.Е.ipynb**. Например: если Вас зовут Иванов Иван Иванович, и Вы обучаетесь в группе 6207_010302D, то имя файла будет выглядеть так **lab_2_6231-010402D_Кришталь К.Е..ipynb**.\n",
    "\n",
    "Необходимо провести исследование различных способов представления документов и их влияние на качество определения тональности.\n",
    "\n",
    "В качестве входных данных к лабораторной работе взят широко известный набор данных IMDB, содержащий 50K обзоров фильмов ([imdb-dataset-of-50k-movie-reviews](https://disk.yandex.ru/i/DDb0zuyUmts5QA)). Откликами являются значения двух классов positive и negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Код загрузки данных\n",
    "# Если хотите добавить какие-либо библиотеки\n",
    "# добавляйте их ИМЕННО ЗДЕСЬ\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "imdb_data = pd.read_csv(r'input/IMDB Dataset.csv')\n",
    "imdb_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг №1 Подготовка данных\n",
    "\n",
    "Обязательно предобработайте данные!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Очищенные отзывы:\n",
      "0    one of the other reviewers has mentioned that ...\n",
      "1    a wonderful little production br br the filmin...\n",
      "2    i thought this was a wonderful way to spend ti...\n",
      "3    basically theres a family where a little boy j...\n",
      "4    petter matteis love in the time of money is a ...\n",
      "Name: cleaned_review, dtype: object\n",
      "Отзывы после удаления стоп-слов:\n",
      "0    one reviewers mentioned watching oz episode yo...\n",
      "1    wonderful little production br br filming tech...\n",
      "2    thought wonderful way spend time hot summer we...\n",
      "3    basically theres family little boy jake thinks...\n",
      "4    petter matteis love time money visually stunni...\n",
      "Name: review_without_stopwords, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Проверка на пустые строки и удаление\n",
    "imdb_data = imdb_data.dropna(subset=['review'])\n",
    "\n",
    "# Функция для очистки текста\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Удаляем цифры, знаки препинания, приводим к нижнему регистру\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # оставляем только буквы\n",
    "    text = text.lower()  # делаем маленькие буквы\n",
    "    return text\n",
    "\n",
    "# Очищаем все отзывы\n",
    "imdb_data['cleaned_review'] = imdb_data['review'].apply(clean_text)\n",
    "\n",
    "\n",
    "print(\"Очищенные отзывы:\")\n",
    "print(imdb_data['cleaned_review'].head())\n",
    "\n",
    "# Удаление стоп-слов\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # разбиваем текст на слова\n",
    "    words_filtered = [word for word in words if word not in stop_words]  # удаляем стоп-слова\n",
    "    return ' '.join(words_filtered)  # обратно в строку\n",
    "\n",
    "# Удаляем стоп-слова\n",
    "imdb_data['review_without_stopwords'] = imdb_data['cleaned_review'].apply(remove_stopwords)\n",
    "\n",
    "# Вывод отзывов после удаления стоп-слов\n",
    "print(\"Отзывы после удаления стоп-слов:\")\n",
    "print(imdb_data['review_without_stopwords'].head())\n",
    "\n",
    "# Лемматизация (если нужно, можно раскомментировать код)\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def lemmatize_text(text):\n",
    "#     words = text.split()  # разбиваем текст на слова\n",
    "#     lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # лемматизируем каждое слово\n",
    "#     return ' '.join(lemmatized_words)  # соединяем обратно в строку\n",
    "\n",
    "# imdb_data['lemmatized_review'] = imdb_data['review_without_stopwords'].apply(lemmatize_text)\n",
    "\n",
    "# Вывод лемматизированных отзывов\n",
    "# print(\"Лемматизированные отзывы:\")\n",
    "# print(imdb_data['lemmatized_review'].head())\n",
    "\n",
    "# imdb_data.head(223)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При векторизации ограничьте количество признаков до 500. В качестве исследуемых способов векторизации текстов необходимо рассмотреть:\n",
    "\n",
    "#### 1. Компоненты вектора: частоты ([CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   able  absolutely  across  act  acting  action  actor  actors  actress  \\\n",
      "0     0           0       0    0       0       0      0       0        0   \n",
      "1     0           0       0    0       0       0      0       1        0   \n",
      "2     0           0       0    0       0       0      0       0        0   \n",
      "3     0           0       0    0       0       0      0       0        0   \n",
      "4     0           0       0    0       1       1      0       0        0   \n",
      "\n",
      "   actually  ...  writing  written  wrong  year  years  yes  yet  youll  \\\n",
      "0         0  ...        0        0      0     0      0    0    0      1   \n",
      "1         0  ...        0        1      0     0      0    0    0      0   \n",
      "2         0  ...        0        0      0     0      1    0    0      0   \n",
      "3         0  ...        0        0      0     0      0    0    0      0   \n",
      "4         0  ...        0        0      0     0      0    0    0      0   \n",
      "\n",
      "   young  youre  \n",
      "0      0      0  \n",
      "1      0      0  \n",
      "2      1      0  \n",
      "3      0      1  \n",
      "4      0      0  \n",
      "\n",
      "[5 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "# вставьте код здесь\n",
    "\n",
    "\n",
    "# Создаем CountVectorizer с ограничением до 500 признаков\n",
    "vectorizer_count = CountVectorizer(max_features=500)\n",
    "\n",
    "# Применяем векторизацию к текстам\n",
    "X_count = vectorizer_count.fit_transform(imdb_data['review_without_stopwords'])\n",
    "\n",
    "# Преобразуем результат в DataFrame для удобства\n",
    "X_count_df = pd.DataFrame(X_count.toarray(), columns=vectorizer_count.get_feature_names_out())\n",
    "\n",
    "# Вывод первых 5 строк DataFrame\n",
    "print(X_count_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Компоненты вектора: оценки tf-idf для слова ([TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   able  absolutely  across  act    acting    action  actor    actors  \\\n",
      "0   0.0         0.0     0.0  0.0  0.000000  0.000000    0.0  0.000000   \n",
      "1   0.0         0.0     0.0  0.0  0.000000  0.000000    0.0  0.112843   \n",
      "2   0.0         0.0     0.0  0.0  0.000000  0.000000    0.0  0.000000   \n",
      "3   0.0         0.0     0.0  0.0  0.000000  0.000000    0.0  0.000000   \n",
      "4   0.0         0.0     0.0  0.0  0.072111  0.097863    0.0  0.000000   \n",
      "\n",
      "   actress  actually  ...  writing   written  wrong  year    years  yes  yet  \\\n",
      "0      0.0       0.0  ...      0.0  0.000000    0.0   0.0  0.00000  0.0  0.0   \n",
      "1      0.0       0.0  ...      0.0  0.151696    0.0   0.0  0.00000  0.0  0.0   \n",
      "2      0.0       0.0  ...      0.0  0.000000    0.0   0.0  0.14788  0.0  0.0   \n",
      "3      0.0       0.0  ...      0.0  0.000000    0.0   0.0  0.00000  0.0  0.0   \n",
      "4      0.0       0.0  ...      0.0  0.000000    0.0   0.0  0.00000  0.0  0.0   \n",
      "\n",
      "      youll     young     youre  \n",
      "0  0.114195  0.000000  0.000000  \n",
      "1  0.000000  0.000000  0.000000  \n",
      "2  0.000000  0.162561  0.000000  \n",
      "3  0.000000  0.000000  0.173071  \n",
      "4  0.000000  0.000000  0.000000  \n",
      "\n",
      "[5 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Применяем TfidfVectorizer к текстам без стоп-слов\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(imdb_data['review_without_stopwords'])\n",
    "\n",
    "# Преобразуем результат в DataFrame\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "print(X_tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Компоненты вектора: частоты N-грам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   absolutely nothing  acting bad  acting good  action movie  action scenes  \\\n",
      "0                   0           0            0             0              0   \n",
      "1                   0           0            0             0              0   \n",
      "2                   0           0            0             0              0   \n",
      "3                   0           0            0             0              0   \n",
      "4                   0           0            1             0              0   \n",
      "\n",
      "   action sequences  againbr br  allbr br  almost every  along way  ...  \\\n",
      "0                 0           0         0             0          0  ...   \n",
      "1                 0           0         0             0          0  ...   \n",
      "2                 0           0         0             0          0  ...   \n",
      "3                 0           0         0             0          0  ...   \n",
      "4                 0           0         0             0          0  ...   \n",
      "\n",
      "   years ago  years later  years old  yet another  young girl  young man  \\\n",
      "0          0            0          0            0           0          0   \n",
      "1          0            0          0            0           0          0   \n",
      "2          0            0          0            0           0          0   \n",
      "3          0            0          0            0           0          0   \n",
      "4          0            0          0            0           0          0   \n",
      "\n",
      "   young woman  youre looking  youve got  youve seen  \n",
      "0            0              0          0           0  \n",
      "1            0              0          0           0  \n",
      "2            0              0          0           0  \n",
      "3            0              0          0           0  \n",
      "4            0              0          0           0  \n",
      "\n",
      "[5 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "# вставьте код здесь\n",
    "\n",
    "# Векторизация текста с униграммами и биграммами\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(2, 2), max_features=500)\n",
    "\n",
    "# Используем 'review_without_stopwords' как очищенные данные\n",
    "X_ngram = vectorizer_ngram.fit_transform(imdb_data['review_without_stopwords'])\n",
    "\n",
    "# Преобразуем результат в DataFrame\n",
    "X_ngram_df = pd.DataFrame(X_ngram.toarray(), columns=vectorizer_ngram.get_feature_names_out())\n",
    "\n",
    "# Вывод первых 5 строк\n",
    "print(X_ngram_df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2. Исследование моделей\n",
    "\n",
    "Матрица ошибок (confusion matrix):\n",
    "\n",
    "<table>\n",
    "\t\t<tr>\n",
    "\t\t\t<td></td>\n",
    "\t\t\t<td>$y = 1$</td>\n",
    "\t\t\t<td>$y = 0$</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>$a(x) = 1$</td>\n",
    "\t\t\t<td>True Positive (TP)</td>\n",
    "\t\t\t<td>False Positive (FP)</td>\n",
    "\t\t</tr>\n",
    "    \t<tr>\n",
    "\t\t\t<td>$a(x) = 0$</td>\n",
    "\t\t\t<td>False Negative (FN)</td>\n",
    "\t\t\t<td>True Negative (TN)</td>\n",
    "\t\t</tr>\n",
    "</table>\n",
    "\n",
    "Метрики качества классификации:\n",
    "\n",
    "$$\\operatorname{accuracy} = \\frac{\\operatorname{TP} + \\operatorname{TN}}{\\operatorname{TP} + \\operatorname{TN} + \\operatorname{FP} + \\operatorname{FN}}$$\n",
    "\n",
    "\n",
    "$$\\operatorname{precision} = \\frac{\\operatorname{TP}}{\\operatorname{TP} + \\operatorname{FP}}$$\n",
    "\n",
    "$$\\operatorname{recall} = \\frac{\\operatorname{TP}}{\\operatorname{TP} + \\operatorname{FN}}$$\n",
    "\n",
    "$$\\operatorname{F} = \\frac{\\operatorname{precision} \\cdot \\operatorname{recall}}{\\operatorname{precision} + \\operatorname{recall}}$$\n",
    "\n",
    "Для каждой модели и каждого способа векторизации текстов необходимо:\n",
    "\n",
    "1. Определить оптимальные гиперпараметры (по F-мере) ([GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).\n",
    "\n",
    "2. Количество блоков при перекрестной проверке (cross-validation) должно быть равно 3.\n",
    "\n",
    "3. Для ускорения процесса можно ограничиться 30% всех данных.\n",
    "\n",
    "Оценку производим для следующих моделей:\n",
    "\n",
    "#### 1. Машина опорных векторов ([SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вставьте код здесь\n",
    "param_grid_svc = {\n",
    "    'C': [0.2, 2, 20],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Если завтра днём не обчучиться модельки на проце , то попробую на видео карте "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# # Векторизация с использованием TF-IDF\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "# X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# # Векторизация с использованием CountVectorizer\n",
    "# count_vectorizer = CountVectorizer(max_features=500)\n",
    "# X_count = count_vectorizer.fit_transform(X)\n",
    "\n",
    "# # Разделение данных на обучающие и тестовые\n",
    "# X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "# X_train_count, X_test_count, y_train_count, y_test_count = train_test_split(X_count, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Преобразование в тензоры для использования с PyTorch и CUDA\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# X_train_tfidf_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32).to(device)\n",
    "# X_test_tfidf_tensor = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32).to(device)\n",
    "# y_train_tensor = torch.tensor(y_train.values).long().to(device)\n",
    "# y_test_tensor = torch.tensor(y_test.values).long().to(device)\n",
    "\n",
    "# X_train_count_tensor = torch.tensor(X_train_count.toarray(), dtype=torch.float32).to(device)\n",
    "# X_test_count_tensor = torch.tensor(X_test_count.toarray(), dtype=torch.float32).to(device)\n",
    "# y_train_count_tensor = torch.tensor(y_train_count.values).long().to(device)\n",
    "# y_test_count_tensor = torch.tensor(y_test_count.values).long().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TextClassifier(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super(TextClassifier, self).__init__()\n",
    "#         self.linear = nn.Linear(input_size, 2)  # 2 выхода для бинарной классификации\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.linear(x)\n",
    "\n",
    "# # Модели для обеих векторизаций\n",
    "# input_size_tfidf = X_train_tfidf_tensor.shape[1]\n",
    "# input_size_count = X_train_count_tensor.shape[1]\n",
    "\n",
    "# model_tfidf = TextClassifier(input_size_tfidf).to(device)\n",
    "# model_count = TextClassifier(input_size_count).to(device)\n",
    "\n",
    "# # Функция потерь и оптимизатор\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer_tfidf = optim.Adam(model_tfidf.parameters(), lr=0.001)\n",
    "# optimizer_count = optim.Adam(model_count.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, optimizer, X_train, y_train, num_epochs=10):\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(X_train)\n",
    "#         loss = criterion(outputs, y_train)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# # Обучение модели с TF-IDF\n",
    "# train_model(model_tfidf, optimizer_tfidf, X_train_tfidf_tensor, y_train_tensor)\n",
    "\n",
    "# # Обучение модели с CountVectorizer\n",
    "# train_model(model_count, optimizer_count, X_train_count_tensor, y_train_count_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, X_test, y_test):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(X_test)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "#     predicted = predicted.cpu()\n",
    "#     y_test_cpu = y_test.cpu()\n",
    "\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(confusion_matrix(y_test_cpu, predicted))\n",
    "#     print(\"Classification Report:\")\n",
    "#     print(classification_report(y_test_cpu, predicted))\n",
    "\n",
    "# print(\"Оценка модели с TF-IDF:\")\n",
    "# evaluate_model(model_tfidf, X_test_tfidf_tensor, y_test_tensor)\n",
    "\n",
    "# print(\"Оценка модели с CountVectorizer:\")\n",
    "# evaluate_model(model_count, X_test_count_tensor, y_test_count_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Случайный лес ([RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Разделение данных (30% тест, 70% обучающие данные)\n",
    "X = imdb_data['review_without_stopwords']  \n",
    "y = imdb_data['sentiment']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Результаты для TF-IDF векторизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- TF-IDF Векторизация -----------\n",
    "print(\"\\nРезультаты для TF-IDF векторизации:\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# ----------- GridSearch для SVC -----------\n",
    "# Параметры для GridSearchCV (для SVC)\n",
    "param_grid_svc = {\n",
    "    'C': [0.2, 2, 20],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Обучение SVC модели с использованием GridSearch\n",
    "svc_model = SVC()\n",
    "grid_search_svc = GridSearchCV(svc_model, param_grid_svc, scoring='f1_macro', cv=2, n_jobs=-1)\n",
    "grid_search_svc.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Лучшая модель SVC\n",
    "svc_best = grid_search_svc.best_estimator_\n",
    "y_pred_svc = svc_best.predict(X_test_tfidf)\n",
    "\n",
    "# Метрики для SVC\n",
    "print(\"\\nКлассификация SVC с TF-IDF:\")\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "print(confusion_matrix(y_test, y_pred_svc))\n",
    "print(\"Лучшие параметры для SVC:\", grid_search_svc.best_params_)\n",
    "print(\"Лучший результат (F1):\", grid_search_svc.best_score_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Результаты для CountVectorizer векторизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------- CountVectorizer Векторизация -----------\n",
    "print(\"\\nРезультаты для CountVectorizer векторизации:\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Создаем модель случайного леса с CountVectorizer\n",
    "rf_model_count = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Выполнение GridSearchCV для CountVectorizer\n",
    "grid_search_rf_count = GridSearchCV(rf_model_count, param_grid_rf, scoring='f1_macro', cv=2, n_jobs=-1)\n",
    "grid_search_rf_count.fit(X_train_count, y_train)\n",
    "\n",
    "# Лучшая модель для CountVectorizer\n",
    "rf_best_count = grid_search_rf_count.best_estimator_\n",
    "y_pred_rf_count = rf_best_count.predict(X_test_count)\n",
    "\n",
    "# Метрики для CountVectorizer\n",
    "print(\"\\nКлассификация RandomForest с CountVectorizer:\")\n",
    "print(classification_report(y_test, y_pred_rf_count))\n",
    "print(confusion_matrix(y_test, y_pred_rf_count))\n",
    "print(\"Лучшие параметры для CountVectorizer:\", grid_search_rf_count.best_params_)\n",
    "print(\"Лучший результат (F1):\", grid_search_rf_count.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты для TF-IDF векторизации:\n",
      "\n",
      "Классификация SVC с TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.83      0.84      7411\n",
      "    positive       0.84      0.86      0.85      7589\n",
      "\n",
      "    accuracy                           0.85     15000\n",
      "   macro avg       0.85      0.85      0.85     15000\n",
      "weighted avg       0.85      0.85      0.85     15000\n",
      "\n",
      "[[6183 1228]\n",
      " [1083 6506]]\n",
      "Лучшие параметры для SVC: {'C': 2, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Лучший результат (F1): 0.8341336659296981\n",
      "\n",
      "Результаты для RandomForest с TF-IDF:\n",
      "\n",
      "Классификация RandomForest с TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.81      0.81      7411\n",
      "    positive       0.81      0.83      0.82      7589\n",
      "\n",
      "    accuracy                           0.82     15000\n",
      "   macro avg       0.82      0.82      0.82     15000\n",
      "weighted avg       0.82      0.82      0.82     15000\n",
      "\n",
      "[[5969 1442]\n",
      " [1311 6278]]\n",
      "Лучшие параметры для RandomForest: {'bootstrap': False, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Лучший результат (F1): 0.8072796277462484\n",
      "\n",
      "Результаты для CountVectorizer векторизации:\n",
      "\n",
      "Классификация RandomForest с CountVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.80      0.81      7411\n",
      "    positive       0.81      0.83      0.82      7589\n",
      "\n",
      "    accuracy                           0.81     15000\n",
      "   macro avg       0.81      0.81      0.81     15000\n",
      "weighted avg       0.81      0.81      0.81     15000\n",
      "\n",
      "[[5948 1463]\n",
      " [1324 6265]]\n",
      "Лучшие параметры для CountVectorizer: {'bootstrap': False, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Лучший результат (F1): 0.8060766288534661\n"
     ]
    }
   ],
   "source": [
    "#вЗЯТЬ 30% ОТ ПРЕДЕДЫЩИХ ДАННЫХ\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X = imdb_data['review_without_stopwords']  \n",
    "y = imdb_data['sentiment']  \n",
    "\n",
    "# 30 на 70\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# ----------- TF-IDF Векторизация -----------\n",
    "print(\"\\nРезультаты для TF-IDF векторизации:\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# ----------- GridSearch для SVC -----------\n",
    "# Параметры для GridSearchCV (для SVC)\n",
    "param_grid_svc = {\n",
    "    'C': [0.2, 2, 20],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Обучение SVC модели с использованием GridSearch\n",
    "svc_model = SVC()\n",
    "grid_search_svc = GridSearchCV(svc_model, param_grid_svc, scoring='f1_macro', cv=2, n_jobs=-1)\n",
    "grid_search_svc.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Лучшая модель SVC\n",
    "svc_best = grid_search_svc.best_estimator_\n",
    "y_pred_svc = svc_best.predict(X_test_tfidf)\n",
    "\n",
    "# Метрики для SVC\n",
    "print(\"\\nКлассификация SVC с TF-IDF:\")\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "print(confusion_matrix(y_test, y_pred_svc))\n",
    "print(\"Лучшие параметры для SVC:\", grid_search_svc.best_params_)\n",
    "print(\"Лучший результат (F1):\", grid_search_svc.best_score_)\n",
    "\n",
    "# ----------- RandomForest для TF-IDF -----------\n",
    "print(\"\\nРезультаты для RandomForest с TF-IDF:\")\n",
    "\n",
    "# Определение параметров для GridSearchCV для RandomForest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Обучение модели RandomForest с использованием GridSearch\n",
    "rf_model_tfidf = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf_tfidf = GridSearchCV(rf_model_tfidf, param_grid_rf, scoring='f1_macro', cv=2, n_jobs=-1)\n",
    "grid_search_rf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Лучшая модель для TF-IDF\n",
    "rf_best_tfidf = grid_search_rf_tfidf.best_estimator_\n",
    "y_pred_rf_tfidf = rf_best_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Метрики для TF-IDF\n",
    "print(\"\\nКлассификация RandomForest с TF-IDF:\")\n",
    "print(classification_report(y_test, y_pred_rf_tfidf))\n",
    "print(confusion_matrix(y_test, y_pred_rf_tfidf))\n",
    "print(\"Лучшие параметры для RandomForest:\", grid_search_rf_tfidf.best_params_)\n",
    "print(\"Лучший результат (F1):\", grid_search_rf_tfidf.best_score_)\n",
    "\n",
    "# ----------- CountVectorizer Векторизация -----------\n",
    "print(\"\\nРезультаты для CountVectorizer векторизации:\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=500)\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Создаем модель случайного леса с CountVectorizer\n",
    "rf_model_count = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Выполнение GridSearchCV для CountVectorizer\n",
    "grid_search_rf_count = GridSearchCV(rf_model_count, param_grid_rf, scoring='f1_macro', cv=2, n_jobs=-1)\n",
    "grid_search_rf_count.fit(X_train_count, y_train)\n",
    "\n",
    "# Лучшая модель для CountVectorizer\n",
    "rf_best_count = grid_search_rf_count.best_estimator_\n",
    "y_pred_rf_count = rf_best_count.predict(X_test_count)\n",
    "\n",
    "# Метрики для CountVectorizer\n",
    "print(\"\\nКлассификация RandomForest с CountVectorizer:\")\n",
    "print(classification_report(y_test, y_pred_rf_count))\n",
    "print(confusion_matrix(y_test, y_pred_rf_count))\n",
    "print(\"Лучшие параметры для CountVectorizer:\", grid_search_rf_count.best_params_)\n",
    "print(\"Лучший результат (F1):\", grid_search_rf_count.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 3. Сравнение результатов\n",
    "\n",
    "1. Сравнить и найти наиболее точные (по F-мере) модель и способ векторизации текстов.\n",
    "\n",
    "2. Обучить полученную модель (с заданными оптимальными гиперпараметрами) на всех данных (80% обучающая выборка, 20% тестовая выборка).\n",
    "\n",
    "3. На тестовой выборке постоить матрицу ошибок (confusion matrix) и оценить качество классификации как accuracy, precision, recall и F-мера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-мера для SVC с TF-IDF: 0.8458618618763898\n",
      "F-мера для RandomForest с TF-IDF: 0.8163887493963605\n",
      "F-мера для RandomForest с CountVectorizer: 0.8141169813392791\n",
      "Наиболее точная модель по F-мере: SVC с TF-IDF с F-мерой 0.8458618618763898\n"
     ]
    }
   ],
   "source": [
    "# вставьте код здесь\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# ----------- Сравнение по F-мере -----------\n",
    "# Сравниваем F-мера для моделей с TF-IDF и CountVectorizer\n",
    "\n",
    "# Вычисляем F-меру для каждой модели\n",
    "f1_svc_tfidf = f1_score(y_test, y_pred_svc, average='macro')\n",
    "f1_rf_tfidf = f1_score(y_test, y_pred_rf_tfidf, average='macro')\n",
    "f1_rf_count = f1_score(y_test, y_pred_rf_count, average='macro')\n",
    "\n",
    "# Вывод F-меры для каждой модели\n",
    "print(f\"F-мера для SVC с TF-IDF: {f1_svc_tfidf}\")\n",
    "print(f\"F-мера для RandomForest с TF-IDF: {f1_rf_tfidf}\")\n",
    "print(f\"F-мера для RandomForest с CountVectorizer: {f1_rf_count}\")\n",
    "\n",
    "# Выбираем модель с наибольшей F-мерой\n",
    "best_model_name = \"\"\n",
    "best_f1_score = max(f1_svc_tfidf, f1_rf_tfidf, f1_rf_count)\n",
    "\n",
    "if best_f1_score == f1_svc_tfidf:\n",
    "    best_model_name = \"SVC с TF-IDF\"\n",
    "elif best_f1_score == f1_rf_tfidf:\n",
    "    best_model_name = \"RandomForest с TF-IDF\"\n",
    "else:\n",
    "    best_model_name = \"RandomForest с CountVectorizer\"\n",
    "\n",
    "print(f\"Наиболее точная модель по F-мере: {best_model_name} с F-мерой {best_f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на 80% обучающая выборка и 20% тестовая выборка\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_name == \"SVC с TF-IDF\":\n",
    "    # Векторизация TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "    X_train_vec = tfidf_vectorizer.fit_transform(X_train_final)\n",
    "    X_test_vec = tfidf_vectorizer.transform(X_test_final)\n",
    "\n",
    "    # Лучшая модель SVC\n",
    "    svc_model = SVC(C=grid_search_svc.best_params_['C'],\n",
    "                    kernel=grid_search_svc.best_params_['kernel'],\n",
    "                    gamma=grid_search_svc.best_params_['gamma'],\n",
    "                    random_state=42)\n",
    "    best_model = svc_model\n",
    "\n",
    "elif best_model_name == \"RandomForest с TF-IDF\":\n",
    "    # Векторизация TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "    X_train_vec = tfidf_vectorizer.fit_transform(X_train_final)\n",
    "    X_test_vec = tfidf_vectorizer.transform(X_test_final)\n",
    "\n",
    "    # Лучшая модель RandomForest\n",
    "    rf_model = RandomForestClassifier(n_estimators=grid_search_rf_tfidf.best_params_['n_estimators'],\n",
    "                                      max_depth=grid_search_rf_tfidf.best_params_['max_depth'],\n",
    "                                      min_samples_split=grid_search_rf_tfidf.best_params_['min_samples_split'],\n",
    "                                      min_samples_leaf=grid_search_rf_tfidf.best_params_['min_samples_leaf'],\n",
    "                                      bootstrap=grid_search_rf_tfidf.best_params_['bootstrap'],\n",
    "                                      random_state=42)\n",
    "    best_model = rf_model\n",
    "\n",
    "else:\n",
    "    # Векторизация CountVectorizer\n",
    "    count_vectorizer = CountVectorizer(max_features=500)\n",
    "    X_train_vec = count_vectorizer.fit_transform(X_train_final)\n",
    "    X_test_vec = count_vectorizer.transform(X_test_final)\n",
    "\n",
    "    # Лучшая модель RandomForest\n",
    "    rf_model = RandomForestClassifier(n_estimators=grid_search_rf_count.best_params_['n_estimators'],\n",
    "                                      max_depth=grid_search_rf_count.best_params_['max_depth'],\n",
    "                                      min_samples_split=grid_search_rf_count.best_params_['min_samples_split'],\n",
    "                                      min_samples_leaf=grid_search_rf_count.best_params_['min_samples_leaf'],\n",
    "                                      bootstrap=grid_search_rf_count.best_params_['bootstrap'],\n",
    "                                      random_state=42)\n",
    "    best_model = rf_model\n",
    "\n",
    "# Обучаем выбранную лучшую модель\n",
    "best_model.fit(X_train_vec, y_train_final)\n",
    "\n",
    "# Прогнозируем на тестовой выборке\n",
    "y_pred_final = best_model.predict(X_test_vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества модели на 20% тестовой выборке:\n",
      "Accuracy: 0.8465\n",
      "Precision: 0.8467158270546882\n",
      "Recall: 0.8463993949391881\n",
      "F-мера: 0.8464416094219827\n",
      "\n",
      "Матрица ошибок:\n",
      "[[4135  826]\n",
      " [ 709 4330]]\n"
     ]
    }
   ],
   "source": [
    "# Прогнозируем на тестовой выборке\n",
    "y_pred_final = best_model.predict(X_test_vec)\n",
    "\n",
    "\n",
    "print(\"\\nОценка качества модели на 20% тестовой выборке:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_final, y_pred_final))\n",
    "print(\"Precision:\", precision_score(y_test_final, y_pred_final, average='macro'))\n",
    "print(\"Recall:\", recall_score(y_test_final, y_pred_final, average='macro'))\n",
    "print(\"F-мера:\", f1_score(y_test_final, y_pred_final, average='macro'))\n",
    "\n",
    "print(\"\\nМатрица ошибок:\")\n",
    "print(confusion_matrix(y_test_final, y_pred_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность P(преподаватель | не я): 0.70\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Исходные тексты\n",
    "texts = [\n",
    "    ['<s>', 'я', 'преподаватель', '</s>'],\n",
    "    ['<s>', 'кот', 'это', 'не', 'я', '</s>']\n",
    "]\n",
    "\n",
    "# Расчет частот для униграмм и биграмм\n",
    "def get_ngrams(texts, n):\n",
    "    ngrams = Counter()\n",
    "    for text in texts:\n",
    "        ngrams.update(zip(*[text[i:] for i in range(n)]))\n",
    "    return ngrams\n",
    "\n",
    "# Число уникальных токенов\n",
    "vocab = set(word for text in texts for word in text)\n",
    "V = len(vocab)\n",
    "\n",
    "# Вычисление частот униграмм и биграмм\n",
    "unigrams = get_ngrams(texts, 1)\n",
    "bigrams = get_ngrams(texts, 2)\n",
    "\n",
    "# Метод для вычисления вероятности биграм\n",
    "def bigram_prob(bigram, unigram_count, bigram_count, V):\n",
    "    first_word, second_word = bigram\n",
    "    return bigram_count.get((first_word, second_word), 0) / unigram_count.get(first_word, 1)\n",
    "\n",
    "# Линейная интерполяция\n",
    "def linear_interpolation(prob1, prob2, prob3, lambda1, lambda2, lambda3):\n",
    "    return lambda1 * prob1 + lambda2 * prob2 + lambda3 * prob3\n",
    "\n",
    "# Вычисление вероятности P(преподаватель | не я)\n",
    "# 1. Без учета интерполяции (максимальное правдоподобие)\n",
    "prob_bi_1 = bigram_prob(('не', 'я'), unigrams, bigrams, V)\n",
    "prob_bi_2 = bigram_prob(('я', 'преподаватель'), unigrams, bigrams, V)\n",
    "\n",
    "# 2. Интерполяция\n",
    "lambda1, lambda2, lambda3 = 0.3, 0.4, 0.3\n",
    "\n",
    "# Линейная интерполяция (например, для более простого случая без третьего компонента)\n",
    "final_prob = linear_interpolation(prob_bi_1, prob_bi_2, 0, lambda1, lambda2, lambda3)\n",
    "\n",
    "# Вывод вероятности\n",
    "print(f\"Вероятность P(преподаватель | не я): {final_prob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перплексия фразы 'это не я преподаватель': 6.33\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Исходные тексты\n",
    "texts = [\n",
    "    ['я', 'преподаватель'],\n",
    "    ['кот', 'это', 'не', 'я'],\n",
    "    ['это', 'сделал', 'не', 'я'],\n",
    "    ['я', 'не', 'люблю', 'петь']\n",
    "]\n",
    "\n",
    "# Фраза для оценки перплексии\n",
    "test_phrase = ['это', 'не', 'я', 'преподаватель']\n",
    "\n",
    "# Функция для получения униграмм\n",
    "def get_unigrams(texts):\n",
    "    unigrams = Counter()\n",
    "    total_words = 0\n",
    "    for text in texts:\n",
    "        unigrams.update(text)\n",
    "        total_words += len(text)\n",
    "    return unigrams, total_words\n",
    "\n",
    "# Получаем униграммы и общее количество слов\n",
    "unigrams, total_words = get_unigrams(texts)\n",
    "\n",
    "# Вычисляем вероятность униграммы\n",
    "def unigram_prob(word, unigram_count, total_words):\n",
    "    return unigram_count.get(word, 0) / total_words\n",
    "\n",
    "# Вычисление логарифма вероятности для фразы\n",
    "log_probs = []\n",
    "for word in test_phrase:\n",
    "    prob = unigram_prob(word, unigrams, total_words)\n",
    "    log_probs.append(math.log(prob) if prob > 0 else float('-inf'))\n",
    "\n",
    "# Средняя логарифмическая вероятность\n",
    "avg_log_prob = sum(log_probs) / len(test_phrase)\n",
    "\n",
    "# Перплексия\n",
    "perplexity = math.exp(-avg_log_prob)\n",
    "\n",
    "# Выводим результат с точностью до 2 знаков после запятой\n",
    "print(f\"Перплексия фразы 'это не я преподаватель': {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
